# 深度学习文档
## 目录

#### 交叉熵

##### 定义
  将交叉熵引入计算语言学消岐领域，采用语句的真实语义作为交叉熵的训练集的先验信息，将机器翻译的语义作为测试集后验信息。计算两者的交叉熵，并以交叉熵指导对歧义的辨识和消除。实例表明，该方法简洁有效．易于计算机自适应实现。交叉熵不失为计算语言学消岐的一种较为有效的工具。
  在信息论中，交叉熵是表示两个概率分布p,q，其中p表示真实分布，q表示非真实分布，在相同的一组事件中，其中，用非真实分布q来表示某个事件发生所需要的平均比特数。

> 交叉熵函数
\[
Cross_Entropy = -\sum_{i=1}^n \sum_{j=1}^m y_{ij} \ln{p_{ij}}
\]
```py
import numpy as np

def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
```

#### 对数几率回归
现在，我们终于要讲解机器学习中最热门和最有用的算法之一，它也是所有机器学习的基石——对数几率回归算法。基本上是这样的：

- 获得数据
- 选择一个随机模型
- 计算误差
- 最小化误差，获得更好的模型
- 完成！

\[
Error_Function = - \frac{1}{m} \sum_{i=1}^m (1-y)(\ln{1-\hat{y}}) + y \ln{\hat{y}}
\]
