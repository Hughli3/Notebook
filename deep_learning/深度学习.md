# 深度学习文档
## 目录

### One-Hot 编码



使用pandas
```py
one_hot_data = pd.get_dummies(data["rank"],prefix="rank") pd.concat([data,pd.get_dummies(data["rank"])],axis = 1)

# One-hot函数
pd.get_dummies()

# 合并函数
pd.concat([a,b],axis = 0 or 1,join='outer')
# objs: series，dataframe或者是panel构成的序列lsit
# axis： 需要合并链接的轴，0是行，1是列
# join：连接的方式 inner，或者outer

one_hot_data = one_hot_data.drop("rank", axis = 1)
# 去掉原来的rank数据
```

python One-Hot源码
```py
def dense_to_one_hot(labels_dense, num_classes):
  """Convert class labels from scalars to one-hot vectors."""
  num_labels = labels_dense.shape[0]
  index_offset = numpy.arange(num_labels) * num_classes
  labels_one_hot = numpy.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot
```

### 交叉熵

##### 定义
  将交叉熵引入计算语言学消岐领域，采用语句的真实语义作为交叉熵的训练集的先验信息，将机器翻译的语义作为测试集后验信息。计算两者的交叉熵，并以交叉熵指导对歧义的辨识和消除。实例表明，该方法简洁有效．易于计算机自适应实现。交叉熵不失为计算语言学消岐的一种较为有效的工具。
  在信息论中，交叉熵是表示两个概率分布p,q，其中p表示真实分布，q表示非真实分布，在相同的一组事件中，其中，用非真实分布q来表示某个事件发生所需要的平均比特数。

> 交叉熵函数
\[
CrossEntropy = -\sum_{i=1}^n \sum_{j=1}^m y_{ij} \ln{p_{ij}}
\]
```py
import numpy as np

def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
```

### 对数几率回归
现在，我们终于要讲解机器学习中最热门和最有用的算法之一，它也是所有机器学习的基石——对数几率回归算法。基本上是这样的：

- 获得数据
- 选择一个随机模型
- 计算误差
- 最小化误差，获得更好的模型
- 完成！

\[
ErrorFunction = - \frac{1}{m} \sum_{i=1}^m (1-y)(\ln{1-\hat{y}}) + y \ln{\hat{y}}
\]

### 梯度下降
> 梯度下降算法背后的准则和数学原理。


#### 梯度计算
为了最小化误差函数，我们需要获得一些导数。我们开始计算误差函数的导数吧。首先要注意的是 s 型函数(sigmoid)具有很完美的导数。即
\[
\sigma'(x)=\sigma(x)(1 - \sigma(x))
\]
原因是，我们可以使用商式计算它：

现在，如果有
m 个点标为 $x (1) ,x (2) , ... ,x (m)$
 , 误差公式是：$E=-\frac{1}{m} \sum_{i=1}^{m}(y_i \ln(\hat{y_i})+(1-y_i)ln(1-\hat{y_i}))$
预测是
yi^=σ(Wx(i)+b).
我们的目标是计算 E, 在点 x=(x1 ,…,xn	 ), 时的梯度（偏导数）
∇E=( ∂w 1​	 ∂​	 E,⋯, ∂w n∂​	 E, ∂b∂ E)

为此，首先我们要计算
∂wj∂y^ .
y^ =σ(Wx+b), 因此：

最后一个等式是因为和中的唯一非常量项相对于
wj 正好是
wjxj , 明显具有导数xj.现在可以计算∂wj∂E
类似的计算将得出：

这个实际上告诉了我们很重要的规则。对于具有坐标(x1​,…,xn​	 ), 的点，标签y, 预测
y^​	 , 该点的误差函数梯度是
(
(
y
−
y
^
)
x
1
,
⋯
,
(
y
−
y
^
)
x
n
,
(
y
−
y
^
)
)
.
((y−
y
^
​	 )x
1
​	 ,⋯,(y−
y
^
​	 )x
n
​	 ,(y−
y
^
​	 )).

总之

∇
E
(
W
,
b
)
=
(
y
−
y
^
)
(
x
1
,
…
,
x
n
,
1
)
.
∇E(W,b)=(y−
y
^
​	 )(x
1
​	 ,…,x
n
​	 ,1).

如果思考下，会发现很神奇。梯度实际上是标量乘以点的坐标！什么是标量？也就是标签和预测直接的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。

如果觉得这听起来像感知器算法，其实并非偶然性！稍后我们将详细了解。
